---
title: "Problem Set 3"
author: William Parker
output: html_notebook
---


```{r libraries}
library(tidyverse)
library(mixtools)
```


**1. Load the state legislative professionalism data from the folder**
```{r}
load("State Leg Prof Data & Codebook/legprof-components.v1.0.RData")

set.seed(1324)
```


# 2. Munge the data
```{r clean_data}
state_names <- x %>%
  filter(sessid == "2009/10") %>% 
  na.omit() %>%
  select(stateabv)

clean_df <- x %>%
  filter(sessid == "2009/10") %>% 
  na.omit() %>%
  select(t_slength, slength, salary_real, expend) %>%
  mutate_all(scale)


head(clean_df)
```

```{r state_names}
head(state_names)
```

# 3. Perform quick EDA visually or numerically and discuss the patterns you see.
```{r EDA_hist}

make_hist_list <- function(clean_df){
  var_names <- names(clean_df)

  plots <- vector('list', length(var_names))

  for (i in seq(1: length(var_names))) {
    x <- var_names[[i]]

    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x)) +
      geom_histogram()
  }
  
  return(plots)
}

hist_list <- make_hist_list(clean_df)

cowplot::plot_grid(plotlist = hist_list)
```
Looking at the histogram each normalized variable individually, the non-normaility of expenditures jumps out. Multiple outliers with very high expenditures and a bunch of states clustered below the mean. 

All 4 variables have significant positive skew with long right tail. Each variable also appears bounded in the negative direction (>-2 on the standardized scale). This makes sense because there is presumably a minimum amount of time, salary, and expenditures required to actually be a state legislature at all. 

```{r EDA_scatter}

make_plot_list <- function(clean_df){
  var_names <- combn(names(clean_df), m =2)

  plots <- vector('list', dim(var_names)[2])


  for (i in seq(1:dim(var_names)[2])) {
    x <- var_names[1,i]
    y <- var_names[2,i]
    
    title <- paste(x, "vs.", y)
  
    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x, y = y)) +
      geom_point() +
      labs(title = title)
  }
  
  return(plots)
}

plot_list <- make_plot_list(clean_df)

cowplot::plot_grid(plotlist = plot_list)
```
Total session length and length of regular sessions and very correlated, not suprisingly. There aren't as many clear linear relationships between the other variables, nor obvious clusters in any of the scatter plots.

# 4. Diagnose clusterability in any way youâ€™d prefer (e.g., sparse sampling, ODI, etc.); display the results and discuss the likelihood that natural, non-random structure exist in these data.

I decide to use ODI
```{r ODI}
seriation::dissplot(clean_df %>% dist())
```

ODI suggests maybe two clusters in the data. I would guess the very large states may be clustered together in the upper left hand corner of the plot.

However compared to the iris ODI, the presence of clusterability is much less clear

# 5. Fit a k-means algorithm to these data and present the results. Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below.

```{r kmeans}
k_2_cluster <- kmeans(clean_df, centers = 2)

tibble(state_names = state_names$stateabv, cluster = k_2_cluster$cluster) %>%
  arrange(cluster)
```
Looks like cluster one is small and contains 6 large states (CA, MA, MI, NY, OH, PA). This is consistent with the ODI.

```{r plot}

library(ggrepel)
make_plot_list_w_clusters <- function(clean_df, state_names, clusters, factor = TRUE){


  var_names <- combn(names(clean_df), m =2)

  combo_number <- dim(var_names)[2]
  plots <- vector('list', dim(var_names)[2])

  if (factor ==TRUE){
      clean_df$cluster <- factor(clusters)
  }
  
  if (factor != TRUE){
    clean_df$cluster <- clusters
  }
  
  clean_df$state_name <- state_names$stateabv

  for (i in seq(1:combo_number)) {
    x <- var_names[1,i]
    y <- var_names[2,i]
    
    title <- paste(x, "vs.", y)
  
    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x, y = y)) +
      geom_label(aes(label = state_name, color = cluster)) +
      labs(title = title) + 
      theme(legend.position = "none")
  }
  
  return(cowplot::plot_grid(plotlist = plots))
}

kmeans_scatter <- make_plot_list_w_clusters(clean_df, state_names, k_2_cluster$cluster)

kmeans_scatter
```

## Internal Validity checks to fiind ideal K
```{r check_assumptions}
library(clValid)

wss <- function(k){
  kmeans(clean_df, k)$tot.withinss
}

dunn_extract <- function(k){
  dunn(Data = data.matrix(clean_df), clusters = kmeans(clean_df, centers = k)$cluster)
}

connect_extract <- function(k){
  connectivity(Data = data.matrix(clean_df), clusters = kmeans(clean_df, centers = k)$cluster)
}

avg_silhouette <- function(k){
  if (k ==1) {
    return(NaN)
  }
  mean(cluster::silhouette(kmeans(clean_df, centers = k)$cluster, dist = dist(clean_df))[,3])
}


k_values <- 1:10

wss_values <- map_dbl(k_values, wss)
dunn_values <- map_dbl(k_values, dunn_extract)
connect_values <- map_dbl(k_values, connect_extract)
sil_values <- map_dbl(k_values, avg_silhouette)


find_ideal_k <- tibble(k = k_values,
                       tot_wss = wss_values,
                       dunn = dunn_values,
                       connectivity = connect_values,
                       average_silhouette = sil_values) %>%
  mutate_all(function(x) ifelse(is.infinite(x), NaN, x)) %>% 
  pivot_longer(cols = c("tot_wss", "dunn", "connectivity", "average_silhouette"), 
               names_to = "validity_measure", 
               values_to = "value")

kmeans_valid <- find_ideal_k %>%
  ggplot(aes(x = k, y = value)) +
  scale_x_continuous(breaks = k_values) +
  geom_point() + geom_line() + facet_wrap(~validity_measure, scales = "free_y")

kmeans_valid
```
For K-means, look like Dunn index is maximized, the average silhouette is maximized, and connectivity minimized at 2- clear that k  =2 is ideal. The total within sum of squares drops dramatically from 1 to 2 clusters so it kind of looks like the elbow is present at 2.



# 6. Fit a Gaussian mixture model via the EM algorithm to these data and present the results. 

**Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below.**

I first tried to fit a multivariate gaussian mixture model to all 4 variables
```{r first attempt}
mvnormalmixEM(data.matrix(clean_df), 
                    k = 2)
```

I suspect that this error had something to do with how highly correlated `t_slength` and `slength` were in the data. so I dropped `t_slength` from the feature space and tried again
```{r fit_GMM}
gmm_k2 <- mvnormalmixEM(data.matrix(clean_df %>% select(-t_slength)), 
                    k = 2)
```

### GMM fitted means
```{r fit_means}

tibble( variable = names(clean_df %>% select(-t_slength)),
  mean_1 = gmm_k2$mu[[1]],
       mean_2 = gmm_k2$mu[[2]])

```
Inspecting the fitted means of the distribution, looks like GMM is identifying a distribution of states with "big" numbers on session length, salary and expenditures and another distribution with low numbers. Similar result to K-means


### GMM covariance matrix
```{r covariance_matirx_1}
gmm_k2$sigma[[1]]
```
Inspecting the estimated variance-covariance matrix $\hat{\Sigma}_1$ for the first component, we see wide variance in each of the three parameters. This matches with the large state group being spread out in feature space

```{r covariance_matirx_2}
gmm_k2$sigma[[2]]
```
Inspecting the estimated variance-covariance matrix $\hat{\Sigma}_2$ for the second component, we see much lower variance (especially in session length and expenditures). This makes sense as graphically this second group of states was much closer togehter in the feature space

```{r responsibilities_1}
gmm_k2$posterior
responsibility_1 <- gmm_k2$posterior[,1]
```
Looking at the posterior responsibilites for each state,the results are essentially equivalent to a hard partition. No state actually represents a "mixture" of the two components.

This becomes apparent when plotting:
```{r plot_GMM}
gmm_plots <- make_plot_list_w_clusters(clean_df, state_names, responsibility_1, factor = FALSE)

gmm_plots
```
Here responsibility is mapped as a continuous color aesthetic, but because all the values are so close ot 0 or 1 it effectively becomes a hard partition.


### comparision to kmeans
```{r}
compare_groups <- 
  tibble(state_name = state_names$stateabv,
         gmm_clusters = round(responsibility_1),
         kmeans_clusters = k_2_cluster$cluster
         ) %>% cbind(clean_df)

compare_groups %>%
  group_by(gmm_clusters) %>%
  count(kmeans_clusters)
```

```{r diff_state}
compare_groups %>%
  filter(gmm_clusters ==0 & kmeans_clusters ==1)
```

Looks like the result of the kmeans and GMM are the same except for Michigan. Comparing the plots, this probably has to do with removing `t_slength` as a feature from the GMM, which Michigan had a high value for.


```{r valid_GMM}
# wss <- function(k){
#   kmeans(clean_df, k)$tot.withinss
# }


valid_GMM <- function(k){
  
  data <- data.matrix(clean_df %>% select(-t_slength))
  
  fit <- mvnormalmixEM(data, k = k) 
  
  gmm_clust <- round(fit$posterior[,1])
  
  dunn_k <- dunn(Data = data, clusters = gmm_clust)
  
  if (dunn_k == Inf){
    dunn_k <- NA
  }
  
  conn_k <- connectivity(Data = data, clusters = gmm_clust)
  
  avg_sil <- ifelse(k ==1, NA, mean(cluster::silhouette(gmm_clust, dist = dist(data))[,3]))
  
 return(c(dunn_k, conn_k, avg_sil))
}

valid_GMM_results <- valid_GMM(2)

valid_GMM_results
```

I was unable to calculate the Dunn index, got a connectivity value of `r round(valid_GMM_results[2], digits = 1)` and an average silhouette of `r round(valid_GMM_results[3], digits = 1)`. These values suggest my GMM has slighlty worse internal validity than K-means.


# 7. Fit one additional partitioning technique of your choice

I choose to fit **DBSCAN**. Here the key paramater is epislon, the neighborhood size

## DBSCAN
```{r plot_results}
db_fit <- dbscan::dbscan(data.matrix(clean_df), eps = 0.5)

db_fit

make_plot_list_w_clusters(clean_df, state_names, db_fit$cluster)
```

```{r ideal_eps}
dist_plot <- dist(clean_df)

tibble(dist = dist_plot) %>%
  arrange(dist) %>%
  mutate( n = row_number()) %>%
  ggplot(aes(x = n, y = dist)) +
  geom_line()
```


```{r vary_eps}
eps_list <- seq(0.25, 1, 0.25)
db_fits <- vector(mode = "list", length = length(eps_list))
db_plots <- vector(mode = "list", length = length(eps_list))
i <- 1
for (e in eps_list) {
  db_fits[[i]] <- dbscan::dbscan(data.matrix(clean_df), eps = e, minPts = 3)
  
  print(db_fits[[i]])
  db_plots[[i]] <- make_plot_list_w_clusters(clean_df, state_names, db_fits[[i]]$cluster)
  i <- i +1
}
```

## epsilon = 0.25
```{r dbscan_result}
db_plots[[1]]
```

## epsilon = 0.5
```{r}
db_plots[[2]]
```


## epsilon = 0.75
```{r}
db_plots[[3]]
```


## epsilon = 1
```{r}
db_plots[[4]]
```

I choose epsilon = 1, under this parametrization DBCSAN has identified one main cluster of states and then a group of "noise" states. Given the high variance in the corresponding component of the GMM, I think it makes sense to consider the outlier states as "noise" rather than a distinct cluster.


Therefore when validtating the DBSCAN, I should just look at the compactness of the one cluster and throw out the noise points

```{r valid_DBSCAN}

valid_DBscan <- function(fit){
  
  data <- clean_df
  
  data$db_clust <- fit$cluster
  
  # data <- data %>%
  #   filter(db_clust ==1)
  
  clusters <- data$db_clust
  
  dist_matrix <- dist(data)
  
  data <- data.matrix(data)

  dunn_k <- dunn(Data = data, clusters = clusters)
  
  if (dunn_k == Inf){
    dunn_k <- NA
  }

  conn_k <- connectivity(Data = data, clusters = clusters)
  
  avg_sil <-mean(cluster::silhouette(clusters, dist = dist_matrix)[,3])
  
 return(c(dunn_k, conn_k, avg_sil))
}

valid_DBscan(db_fits[[4]])

```

Considering the noise points as a cluster, DBSCAN has lower connectivity and higher average silhouette than either kmeans or GMM, implying this has the best internal validity.



