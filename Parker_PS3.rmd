---
title: "Problem Set 3"
author: "William Parker"
output:
  pdf_document:
    toc: yes
  html_notebook:
    toc: yes
    toc_float: yes
---


```{r libraries}
library(tidyverse)
library(mixtools)
```

# 1. Load the state legislative professionalism data from the folder
```{r}
load("State Leg Prof Data & Codebook/legprof-components.v1.0.RData")

set.seed(1324)
```


# 2. Munge the data
```{r clean_data}
state_names <- x %>%
  filter(sessid == "2009/10") %>% 
  na.omit() %>%
  select(stateabv)

clean_df <- x %>%
  filter(sessid == "2009/10") %>% 
  na.omit() %>%
  select(t_slength, slength, salary_real, expend) %>%
  mutate_all(scale)


head(clean_df)
```

```{r state_names}
head(state_names)
```

# 3. Perform quick EDA visually or numerically and discuss the patterns you see.
```{r EDA_hist}

make_hist_list <- function(clean_df){
  var_names <- names(clean_df)

  plots <- vector('list', length(var_names))

  for (i in seq(1: length(var_names))) {
    x <- var_names[[i]]

    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x)) +
      geom_histogram()
  }
  
  return(plots)
}

hist_list <- make_hist_list(clean_df)

cowplot::plot_grid(plotlist = hist_list)
```
Looking at the histogram each normalized variable individually, the non-normaility of expenditures jumps out. Multiple outliers with very high expenditures and a bunch of states clustered below the mean. 

All 4 variables have significant positive skew with long right tail. Each variable also appears bounded in the negative direction (>-2 on the standardized scale). This makes sense because there is presumably a minimum amount of time, salary, and expenditures required to actually be a state legislature at all. 

```{r EDA_scatter}

make_plot_list <- function(clean_df){
  var_names <- combn(names(clean_df), m =2)

  plots <- vector('list', dim(var_names)[2])


  for (i in seq(1:dim(var_names)[2])) {
    x <- var_names[1,i]
    y <- var_names[2,i]
    
    title <- paste(x, "vs.", y)
  
    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x, y = y)) +
      geom_point() +
      labs(title = title)
  }
  
  return(plots)
}

plot_list <- make_plot_list(clean_df)

cowplot::plot_grid(plotlist = plot_list)
```
Total session length and length of regular sessions and very correlated, not suprisingly. There aren't as many clear linear relationships between the other variables, nor obvious clusters in any of the scatter plots aside from a group of states with values slightly below the mean in all variables.

# 4. Diagnose clusterability in any way you’d prefer (e.g., sparse sampling, ODI, etc.); display the results and discuss the likelihood that natural, non-random structure exist in these data.

I decide to use ODI
```{r ODI}
seriation::dissplot(clean_df %>% dist())
```

ODI suggests maybe two clusters in the data. I would guess the very large states may be clustered together in the upper left hand corner of the plot.

However compared to the iris ODI, the presence of clusterability is much less clear


\pagebreak
# 5. Fit a k-means algorithm to these data and present the results. 

**Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below.**

```{r kmeans}
k_2_cluster <- kmeans(clean_df, centers = 2)

tibble(state_names = state_names$stateabv, cluster = k_2_cluster$cluster) %>%
  arrange(cluster)
```
Looks like cluster one is small and contains 6 large states (CA, MA, MI, NY, OH, PA). This is consistent with the ODI.

```{r plot}

library(ggrepel)
make_plot_list_w_clusters <- function(clean_df, state_names, clusters, factor = TRUE){


  var_names <- combn(names(clean_df), m =2)

  combo_number <- dim(var_names)[2]
  plots <- vector('list', dim(var_names)[2])

  if (factor ==TRUE){
      clean_df$cluster <- factor(clusters)
  }
  
  if (factor != TRUE){
    clean_df$cluster <- clusters
  }
  
  clean_df$state_name <- state_names$stateabv

  for (i in seq(1:combo_number)) {
    x <- var_names[1,i]
    y <- var_names[2,i]
    
    title <- paste(x, "vs.", y)
  
    plots[[i]] <- clean_df %>%
      ggplot(aes_string(x = x, y = y)) +
      geom_label(aes(label = state_name, color = cluster), size = 2) +
      labs(title = title) + 
      theme(legend.position = "none")
  }
  
  return(cowplot::plot_grid(plotlist = plots))
}

kmeans_scatter <- make_plot_list_w_clusters(clean_df, state_names, k_2_cluster$cluster)

kmeans_scatter
```

## Internal Validity checks to fiind ideal K
```{r check_assumptions}
library(clValid)

wss <- function(k){
  kmeans(clean_df, k)$tot.withinss
}

dunn_extract <- function(k){
  dunn(Data = data.matrix(clean_df), clusters = kmeans(clean_df, centers = k)$cluster)
}

connect_extract <- function(k){
  connectivity(Data = data.matrix(clean_df), clusters = kmeans(clean_df, centers = k)$cluster)
}

avg_silhouette <- function(k){
  if (k ==1) {
    return(NaN)
  }
  mean(cluster::silhouette(kmeans(clean_df, centers = k)$cluster, dist = dist(clean_df))[,3])
}


k_values <- 1:10

wss_values <- map_dbl(k_values, wss)
dunn_values <- map_dbl(k_values, dunn_extract)
connect_values <- map_dbl(k_values, connect_extract)
sil_values <- map_dbl(k_values, avg_silhouette)


find_ideal_k <- tibble(k = k_values,
                       tot_wss = wss_values,
                       dunn = dunn_values,
                       connectivity = connect_values,
                       average_silhouette = sil_values) %>%
  mutate_all(function(x) ifelse(is.infinite(x), NaN, x)) %>% 
  pivot_longer(cols = c("tot_wss", "dunn", "connectivity", "average_silhouette"), 
               names_to = "validity_measure", 
               values_to = "value")

kmeans_valid <- find_ideal_k %>%
  ggplot(aes(x = k, y = value)) +
  scale_x_continuous(breaks = k_values) +
  geom_point() + geom_line() + facet_wrap(~validity_measure, scales = "free_y")

kmeans_valid
```
For K-means, look like Dunn index is maximized, the average silhouette is maximized, and connectivity minimized at 2- clear that k  =2 is ideal. The total within sum of squares drops dramatically from 1 to 2 clusters so it kind of looks like the elbow is present at 2.


\pagebreak
# 6. Fit a Gaussian mixture model via the EM algorithm to these data and present the results. 

**Give a quick, high level summary of the output and general patterns. Initialize the algorithm at k=2, and then check this assumption in the validation questions below.**

I first tried to fit a multivariate gaussian mixture model to all 4 variables
```{r first attempt}
mvnormalmixEM(data.matrix(clean_df), 
                    k = 2)
```

I suspect that this error had something to do with how highly correlated `t_slength` and `slength` were in the data. so I dropped `t_slength` from the feature space and tried again
```{r fit_GMM}
gmm_k2 <- mvnormalmixEM(data.matrix(clean_df %>% select(-t_slength)), 
                    k = 2)
```

### GMM fitted means
```{r fit_means}
tibble( variable = names(clean_df %>% select(-t_slength)),
  mean_1 = gmm_k2$mu[[1]],
       mean_2 = gmm_k2$mu[[2]])
```
Inspecting the fitted means of the distribution, looks like GMM is identifying a distribution of states with "big" numbers on session length, salary and expenditures and another distribution with low numbers. Similar result to K-means


### GMM covariance matrix
```{r covariance_matirx_1}
gmm_k2$sigma[[1]]
```
Inspecting the estimated variance-covariance matrix $\hat{\Sigma}_1$ for the first component, we see wide variance in each of the three parameters. This matches with the large state group being spread out in feature space

```{r covariance_matirx_2}
gmm_k2$sigma[[2]]
```
Inspecting the estimated variance-covariance matrix $\hat{\Sigma}_2$ for the second component, we see much lower variance (especially in session length and expenditures). This makes sense as graphically this second group of states was much closer togehter in the feature space

```{r responsibilities_1}
gmm_k2$posterior
responsibility_1 <- gmm_k2$posterior[,1]
```
Looking at the posterior responsibilites for each state,the results are essentially equivalent to a hard partition. No state actually represents a "mixture" of the two components.

This becomes apparent when plotting:
```{r plot_GMM}
gmm_plots <- make_plot_list_w_clusters(clean_df, state_names, responsibility_1, factor = FALSE)

gmm_plots
```
Here responsibility is mapped as a continuous color aesthetic, but because all the values are so close ot 0 or 1 it effectively becomes a hard partition.


### comparision to kmeans
```{r}
compare_groups <- 
  tibble(state_name = state_names$stateabv,
         gmm_clusters = round(responsibility_1),
         kmeans_clusters = k_2_cluster$cluster
         ) %>% cbind(clean_df)

compare_groups %>%
  group_by(gmm_clusters) %>%
  count(kmeans_clusters)
```

```{r diff_state}
compare_groups %>%
  filter(gmm_clusters ==0 & kmeans_clusters ==1)
```

Looks like the result of the kmeans and GMM are the same except for Michigan. Comparing the plots, this probably has to do with removing `t_slength` as a feature from the GMM, which Michigan had a high value for.


For internal validity I selected connectivity and the average silhouette to compare kmeans to GMM.
```{r valid_GMM}

valid_GMM <- function(k){
  
  data <- data.matrix(clean_df %>% select(-t_slength))
  
  fit <- mvnormalmixEM(data, k = k) 
  
  gmm_clust <- round(fit$posterior[,1])

  
  conn_k <- connectivity(Data = data, clusters = gmm_clust)
  
  avg_sil <- ifelse(k ==1, NA, mean(cluster::silhouette(gmm_clust, dist = dist(data))[,3]))
  
 return(c(conn_k, avg_sil))
}

valid_GMM_results <- valid_GMM(2)

valid_GMM_results
```

I got a connectivity value of `r round(valid_GMM_results[1], digits = 1)` and an average silhouette of `r round(valid_GMM_results[2], digits = 1)`. These values suggest my GMM has worse internal validity than K-means.

\pagebreak
# 7. Fit one additional partitioning technique of your choice

## DBSCAN
I choose to fit **DBSCAN**. Here the key paramater is epislon, the neighborhood size. I read that you can guess epislon by plotting the nearest neighbor distances and finding the inflection point

```{r ideal_eps}
dist_plot <- dist(clean_df)

nn_dist <- apply(data.matrix(dist_plot), 1, FUN = function(x) {min(x[x > 0])})
 
states_w_nn <- tibble(nn_dist = as.numeric(nn_dist),
       state = state_names$stateabv) %>%
  arrange(nn_dist) %>%
  mutate( n = row_number())

ggplot(data = states_w_nn, aes(x = n, y = nn_dist)) +
  geom_point() + 
  geom_label_repel(data = states_w_nn %>% filter(nn_dist > 0.5), aes(label = state)) + 
  labs(x = "state index", y = "nearest neighboor distance")
```
From this plot looks like epsilon 0.5 or epsilon of 1 are reasonable choices

```{r vary_eps}
eps_list <- seq(0.25, 1.25, 0.25)
db_fits <- vector(mode = "list", length = length(eps_list))
db_plots <- vector(mode = "list", length = length(eps_list))
i <- 1
for (e in eps_list) {
  db_fits[[i]] <- dbscan::dbscan(data.matrix(clean_df), eps = e, minPts = 3)
  
  print(db_fits[[i]])
  db_plots[[i]] <- make_plot_list_w_clusters(clean_df, state_names, db_fits[[i]]$cluster)
  i <- i +1
}
```

\pagebreak
## epsilon = 0.25
```{r dbscan_result}
db_plots[[1]]
```

\pagebreak
## epsilon = 0.5
```{r}
db_plots[[2]]
```

\pagebreak
## epsilon = 0.75
```{r}
db_plots[[3]]
```

\pagebreak
## epsilon = 1
```{r}
db_plots[[4]]
```
\pagebreak
## epsilon = 1.25
```{r}
db_plots[[5]]
```


I choose epsilon = 1, under this parametrization DBCSAN has identified one main cluster of states and then a group of "noise" states that is visually appealing. Given the high variance in the corresponding component of the GMM, I think it makes sense to consider the outlier states as "noise" rather than a distinct cluster.

Therefore when validtating the DBSCAN, if could be argued that should just apply the measurees of validity to the one cluster and throw out the noise points. However to compare apples to apples, I'll treat the noise as a cluster.

```{r valid_DBSCAN}

valid_DBscan <- function(fit){
  
  data <- clean_df
  
  data$db_clust <- fit$cluster
  
  # data <- data %>%
  #   filter(db_clust ==1)
  
  clusters <- data$db_clust
  
  dist_matrix <- dist(data)
  
  data <- data.matrix(data)

  conn_k <- connectivity(Data = data, clusters = clusters)
  
  avg_sil <-mean(cluster::silhouette(clusters, dist = dist_matrix)[,3])
  
 return(c(conn_k, avg_sil))
}

valid_DBscan(db_fits[[4]])

```

Even if we treat the noise points as a cluster, DBSCAN has lower connectivity and higher average silhouette than either kmeans or GMM, implying this has the method has the best internal validity.

\pagebreak
# 9. Select a single validation strategy (e.g., compactness via min(WSS), average silhouette width, etc.), and calculate for all three algorithms.

As documented thoughout above, I used connectivity and average silhouette width to compare approaches. These measures both indicated that DBSCAN outperformed kmeans (which both beat GMM).

# 10. Discuss the validation output.

## a. What can you take away from the fit?
 
Thinking about the results of all 3 approaches as a whole, I believe the results suggest that most state legislatures operate in similar fashion (expenditures, session length, salary) but there are 5-8 "outlier" states with large deviations in one or more variables. the outlier states aren't really that similar across the variables.

## b. Which approach is optimal? And optimal at what value of k?

The DBSCAN apporach was optimmal, as it was well deisgned for this scenario where we had one big cluster of tightly grouped states in the feature space and then several noise outlier states (that were scattered about). The internal validation measures I chose confirm this.

## c. What are reasons you could imagine selecting a technically “sub-optimal” partitioning method, regardless of the validation statistics?

I think DBSCAN would be the right approach here in a cluster vs. outlier scenario even if the "noise" cluster made the clustering less compact overall by the internal validation statistics. In fact, I think one could argue that only the DBSCAN identified clusters should be evaluated, as the algorithim sets noise points aside as outside the main data generating process.

Conversely, in a scenario where noise/outliers don't make sense (or I really wanted to definitively cluster all the data points) I would not choose DBSCAN even if the internal validation results were better. 